{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd2b1c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcd2b1c5",
    "outputId": "c0e79204-0c15-4665-9a8a-e070309c3ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "from torchvision.models.resnet import resnet18, ResNet18_Weights\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Enhanced Data Augmentation and Normalization for CIFAR-10\n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "}\n",
    "# Load CIFAR-10 Dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform['train'])\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d13e9f",
   "metadata": {
    "id": "b4d13e9f"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class IntermediateBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, num_convs):\n",
    "#         super(IntermediateBlock, self).__init__()\n",
    "#         self.convs = nn.ModuleList([\n",
    "#             nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, padding=1)\n",
    "#             for i in range(num_convs)\n",
    "#         ])\n",
    "#         # The Linear layer should match the number of output channels from the conv layers\n",
    "#         self.fc = nn.Linear(out_channels, num_convs)  # Fully connected layer for generating 'a'\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for conv in self.convs:\n",
    "#             x = F.relu(conv(x))\n",
    "\n",
    "#         m = torch.mean(x, dim=[2, 3])  # Mean across spatial dimensions\n",
    "#         a = F.softmax(self.fc(m), dim=1)  # Generate weights 'a'\n",
    "\n",
    "#         x_prime = torch.zeros_like(x)\n",
    "#         for i, conv in enumerate(self.convs):\n",
    "#             conv_output = conv(x)\n",
    "#             weight = a[:, i].unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
    "#             x_prime += weight * conv_output\n",
    "\n",
    "#         return x_prime\n",
    "\n",
    "\n",
    "# class OutputBlock(nn.Module):\n",
    "#     def __init__(self, in_features, num_classes, hidden_layers=[]):\n",
    "#         super(OutputBlock, self).__init__()\n",
    "#         self.layers = nn.Sequential()\n",
    "\n",
    "#         # Set the correct input size\n",
    "#         input_size = in_features  # This should be 8192 given the ResNet structure for CIFAR-10\n",
    "\n",
    "#         # Create layers\n",
    "#         for hidden_size in hidden_layers:\n",
    "#             self.layers.add_module('fc', nn.Linear(input_size, hidden_size))\n",
    "#             self.layers.add_module('relu', nn.ReLU(inplace=True))\n",
    "#             input_size = hidden_size\n",
    "        \n",
    "#         # Final classification layer\n",
    "#         self.layers.add_module('fc_final', nn.Linear(input_size, num_classes))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.flatten(x, 1)  # Flatten the tensor\n",
    "#         x = self.layers(x)       # Pass through the layers\n",
    "#         return x\n",
    "\n",
    "# # Adjustments in the ResNet modification function\n",
    "# def modify_resnet18_for_cifar10_with_custom_blocks():\n",
    "#     resnet18 = models.resnet18(pretrained=False)\n",
    "#     resnet18.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     resnet18.maxpool = nn.Identity()\n",
    " \n",
    "#     # Insert IntermediateBlock after layer1 of ResNet (for example)\n",
    "#     in_channels = resnet18.layer1[-1].conv2.out_channels  # Get the number of output channels from the last conv layer in layer1\n",
    "#     resnet18.layer1.add_module(\"intermediate_block\", IntermediateBlock(in_channels, in_channels, num_convs=2))\n",
    "\n",
    "#     # Assuming the output size before the fc layer is 8192 after flattening\n",
    "#     in_features = 8192\n",
    "#     resnet18.fc = OutputBlock(in_features, num_classes=10, hidden_layers=[512, 256])\n",
    "\n",
    "#     return resnet18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07db2c44-97d0-4b46-9e07-d8e0b1aac624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IntermediateBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_convs):\n",
    "        super(IntermediateBlock, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "            for _ in range(num_convs)\n",
    "        ])\n",
    "        self.fc = nn.Linear(in_channels, num_convs)  # Fully connected layer for generating 'a'\n",
    "\n",
    "    def forward(self, x):\n",
    "        m = torch.mean(x, dim=[2, 3])  # Mean across spatial dimensions\n",
    "        a = F.softmax(self.fc(m), dim=1)  # Generate weights 'a'\n",
    "\n",
    "        x_prime = None  # Initialize x_prime after the first conv operation\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            conv_output = conv(x)\n",
    "            if x_prime is None:\n",
    "                x_prime = torch.zeros_like(conv_output)\n",
    "            weight = a[:, i].unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
    "            x_prime += weight * conv_output\n",
    "\n",
    "        return x_prime\n",
    "\n",
    "\n",
    "class OutputBlock(nn.Module):\n",
    "    def __init__(self, in_features, num_classes, hidden_layers=[]):\n",
    "        super(OutputBlock, self).__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "\n",
    "        # Set the correct input size\n",
    "        input_size = in_features  # This should be 8192 given the ResNet structure for CIFAR-10\n",
    "\n",
    "        # Create layers\n",
    "        for hidden_size in hidden_layers:\n",
    "            self.layers.add_module('fc', nn.Linear(input_size, hidden_size))\n",
    "            self.layers.add_module('relu', nn.ReLU(inplace=True))\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.layers.add_module('fc_final', nn.Linear(input_size, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)  # Flatten the tensor\n",
    "        x = self.layers(x)       # Pass through the layers\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0b96f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db0b96f2",
    "outputId": "50cd930e-849c-4973-8eb3-4682042cb3d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 200\n",
    "learning_rate = 0.001\n",
    "batch_size = 128 \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class CustomCIFAR10Model(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCIFAR10Model, self).__init__()\n",
    "        # IntermediateBlock processes the input images first\n",
    "        self.intermediate_block = IntermediateBlock(3, 64, num_convs=2)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer\n",
    "\n",
    "        # OutputBlock generates a class-dimension vector\n",
    "        self.output_block = OutputBlock(65536, num_classes=num_classes)  # Assuming the IntermediateBlock output size is 65536\n",
    "\n",
    "        # ResNet18 model for feature extraction\n",
    "        self.resnet18 = models.resnet18(pretrained=False)\n",
    "        self.resnet18.fc = nn.Identity()  # Remove the final FC layer to use features\n",
    "\n",
    "        # Linear layer to combine ResNet18 features with class-dimension vector\n",
    "        self.combine_fc = nn.Linear(512 + num_classes, num_classes)  # 512 from ResNet18 features, num_classes from class-dimension vector\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the IntermediateBlock\n",
    "        intermediate = self.intermediate_block(x)\n",
    "        intermediate = self.dropout(intermediate)  # Apply dropout\n",
    "\n",
    "        # Flatten and pass through the OutputBlock to get class-dimension vector\n",
    "        class_vector = self.output_block(torch.flatten(intermediate, 1))\n",
    "        \n",
    "        # Extract features from ResNet18\n",
    "        features = self.resnet18(x)\n",
    "        \n",
    "        # Concatenate ResNet18 features with class-dimension vector\n",
    "        combined = torch.cat((features, class_vector), dim=1)\n",
    "        \n",
    "        # Pass through the final linear layer for classification\n",
    "        out = self.combine_fc(combined)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Assuming train_loader and test_loader are defined and loaded with the updated transform\n",
    "model = CustomCIFAR10Model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # Added weight decay\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a019f5-15b1-4039-a74b-597217600128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cbae9d9",
   "metadata": {
    "id": "3cbae9d9"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    train_loss /= len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'\\nTrain set: Average loss: {train_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.2f}%)')\n",
    "\n",
    "def evaluate(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            test_loss += criterion(outputs, targets).item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.2f}%)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa7905",
   "metadata": {
    "id": "83aa7905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1702400440653/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 1.6132, Accuracy: 20665/50000 (41.33%)\n",
      "Test set: Average loss: 1.3051, Accuracy: 5301/10000 (53.01%)\n",
      "--------------------------------\n",
      "Epoch: 2/200\n",
      "\n",
      "Train set: Average loss: 1.2975, Accuracy: 26643/50000 (53.29%)\n",
      "Test set: Average loss: 1.1554, Accuracy: 5923/10000 (59.23%)\n",
      "--------------------------------\n",
      "Epoch: 3/200\n",
      "\n",
      "Train set: Average loss: 1.1476, Accuracy: 29658/50000 (59.32%)\n",
      "Test set: Average loss: 1.0044, Accuracy: 6449/10000 (64.49%)\n",
      "--------------------------------\n",
      "Epoch: 4/200\n",
      "\n",
      "Train set: Average loss: 1.0397, Accuracy: 31593/50000 (63.19%)\n",
      "Test set: Average loss: 0.9249, Accuracy: 6675/10000 (66.75%)\n",
      "--------------------------------\n",
      "Epoch: 5/200\n",
      "\n",
      "Train set: Average loss: 0.9688, Accuracy: 33040/50000 (66.08%)\n",
      "Test set: Average loss: 0.8416, Accuracy: 7031/10000 (70.31%)\n",
      "--------------------------------\n",
      "Epoch: 6/200\n",
      "\n",
      "Train set: Average loss: 0.9143, Accuracy: 33988/50000 (67.98%)\n",
      "Test set: Average loss: 0.8343, Accuracy: 7067/10000 (70.67%)\n",
      "--------------------------------\n",
      "Epoch: 7/200\n",
      "\n",
      "Train set: Average loss: 0.8747, Accuracy: 34703/50000 (69.41%)\n",
      "Test set: Average loss: 0.7978, Accuracy: 7198/10000 (71.98%)\n",
      "--------------------------------\n",
      "Epoch: 8/200\n",
      "\n",
      "Train set: Average loss: 0.8369, Accuracy: 35456/50000 (70.91%)\n",
      "Test set: Average loss: 0.7790, Accuracy: 7309/10000 (73.09%)\n",
      "--------------------------------\n",
      "Epoch: 9/200\n",
      "\n",
      "Train set: Average loss: 0.8057, Accuracy: 36104/50000 (72.21%)\n",
      "Test set: Average loss: 0.7317, Accuracy: 7509/10000 (75.09%)\n",
      "--------------------------------\n",
      "Epoch: 10/200\n",
      "\n",
      "Train set: Average loss: 0.7736, Accuracy: 36602/50000 (73.20%)\n",
      "Test set: Average loss: 0.6841, Accuracy: 7629/10000 (76.29%)\n",
      "--------------------------------\n",
      "Epoch: 11/200\n",
      "\n",
      "Train set: Average loss: 0.7499, Accuracy: 37007/50000 (74.01%)\n",
      "Test set: Average loss: 0.6933, Accuracy: 7597/10000 (75.97%)\n",
      "--------------------------------\n",
      "Epoch: 12/200\n",
      "\n",
      "Train set: Average loss: 0.7366, Accuracy: 37189/50000 (74.38%)\n",
      "Test set: Average loss: 0.6839, Accuracy: 7639/10000 (76.39%)\n",
      "--------------------------------\n",
      "Epoch: 13/200\n",
      "\n",
      "Train set: Average loss: 0.7144, Accuracy: 37676/50000 (75.35%)\n",
      "Test set: Average loss: 0.6542, Accuracy: 7751/10000 (77.51%)\n",
      "--------------------------------\n",
      "Epoch: 14/200\n",
      "\n",
      "Train set: Average loss: 0.6977, Accuracy: 37959/50000 (75.92%)\n",
      "Test set: Average loss: 0.7043, Accuracy: 7588/10000 (75.88%)\n",
      "--------------------------------\n",
      "Epoch: 15/200\n",
      "\n",
      "Train set: Average loss: 0.6839, Accuracy: 38177/50000 (76.35%)\n",
      "Test set: Average loss: 0.7421, Accuracy: 7522/10000 (75.22%)\n",
      "--------------------------------\n",
      "Epoch: 16/200\n",
      "\n",
      "Train set: Average loss: 0.6728, Accuracy: 38486/50000 (76.97%)\n",
      "Test set: Average loss: 0.6630, Accuracy: 7698/10000 (76.98%)\n",
      "--------------------------------\n",
      "Epoch: 17/200\n",
      "\n",
      "Train set: Average loss: 0.6574, Accuracy: 38560/50000 (77.12%)\n",
      "Test set: Average loss: 0.6795, Accuracy: 7657/10000 (76.57%)\n",
      "--------------------------------\n",
      "Epoch: 18/200\n",
      "\n",
      "Train set: Average loss: 0.6393, Accuracy: 38911/50000 (77.82%)\n",
      "Test set: Average loss: 0.6136, Accuracy: 7889/10000 (78.89%)\n",
      "--------------------------------\n",
      "Epoch: 19/200\n",
      "\n",
      "Train set: Average loss: 0.6288, Accuracy: 39083/50000 (78.17%)\n",
      "Test set: Average loss: 0.6577, Accuracy: 7752/10000 (77.52%)\n",
      "--------------------------------\n",
      "Epoch: 20/200\n",
      "\n",
      "Train set: Average loss: 0.6189, Accuracy: 39302/50000 (78.60%)\n",
      "Test set: Average loss: 0.6037, Accuracy: 7914/10000 (79.14%)\n",
      "--------------------------------\n",
      "Epoch: 21/200\n",
      "\n",
      "Train set: Average loss: 0.6148, Accuracy: 39416/50000 (78.83%)\n",
      "Test set: Average loss: 0.5712, Accuracy: 8064/10000 (80.64%)\n",
      "--------------------------------\n",
      "Epoch: 22/200\n",
      "\n",
      "Train set: Average loss: 0.5999, Accuracy: 39624/50000 (79.25%)\n",
      "Test set: Average loss: 0.5724, Accuracy: 8074/10000 (80.74%)\n",
      "--------------------------------\n",
      "Epoch: 23/200\n",
      "\n",
      "Train set: Average loss: 0.5903, Accuracy: 39820/50000 (79.64%)\n",
      "Test set: Average loss: 0.5742, Accuracy: 8043/10000 (80.43%)\n",
      "--------------------------------\n",
      "Epoch: 24/200\n",
      "\n",
      "Train set: Average loss: 0.5813, Accuracy: 39951/50000 (79.90%)\n",
      "Test set: Average loss: 0.5740, Accuracy: 8032/10000 (80.32%)\n",
      "--------------------------------\n",
      "Epoch: 25/200\n",
      "\n",
      "Train set: Average loss: 0.5710, Accuracy: 40054/50000 (80.11%)\n",
      "Test set: Average loss: 0.5602, Accuracy: 8078/10000 (80.78%)\n",
      "--------------------------------\n",
      "Epoch: 26/200\n",
      "\n",
      "Train set: Average loss: 0.5659, Accuracy: 40259/50000 (80.52%)\n",
      "Test set: Average loss: 0.5369, Accuracy: 8171/10000 (81.71%)\n",
      "--------------------------------\n",
      "Epoch: 27/200\n",
      "\n",
      "Train set: Average loss: 0.5606, Accuracy: 40227/50000 (80.45%)\n",
      "Test set: Average loss: 0.5598, Accuracy: 8103/10000 (81.03%)\n",
      "--------------------------------\n",
      "Epoch: 28/200\n",
      "\n",
      "Train set: Average loss: 0.5517, Accuracy: 40462/50000 (80.92%)\n",
      "Test set: Average loss: 0.5472, Accuracy: 8176/10000 (81.76%)\n",
      "--------------------------------\n",
      "Epoch: 29/200\n",
      "\n",
      "Train set: Average loss: 0.5416, Accuracy: 40602/50000 (81.20%)\n",
      "Test set: Average loss: 0.5821, Accuracy: 8072/10000 (80.72%)\n",
      "--------------------------------\n",
      "Epoch: 30/200\n",
      "\n",
      "Train set: Average loss: 0.5405, Accuracy: 40679/50000 (81.36%)\n",
      "Test set: Average loss: 0.5379, Accuracy: 8212/10000 (82.12%)\n",
      "--------------------------------\n",
      "Epoch: 31/200\n",
      "\n",
      "Train set: Average loss: 0.4430, Accuracy: 42337/50000 (84.67%)\n",
      "Test set: Average loss: 0.4577, Accuracy: 8441/10000 (84.41%)\n",
      "--------------------------------\n",
      "Epoch: 32/200\n",
      "\n",
      "Train set: Average loss: 0.4170, Accuracy: 42825/50000 (85.65%)\n",
      "Test set: Average loss: 0.4529, Accuracy: 8493/10000 (84.93%)\n",
      "--------------------------------\n",
      "Epoch: 33/200\n",
      "\n",
      "Train set: Average loss: 0.4007, Accuracy: 43031/50000 (86.06%)\n",
      "Test set: Average loss: 0.4523, Accuracy: 8486/10000 (84.86%)\n",
      "--------------------------------\n",
      "Epoch: 34/200\n",
      "\n",
      "Train set: Average loss: 0.3968, Accuracy: 43082/50000 (86.16%)\n",
      "Test set: Average loss: 0.4400, Accuracy: 8535/10000 (85.35%)\n",
      "--------------------------------\n",
      "Epoch: 35/200\n",
      "\n",
      "Train set: Average loss: 0.3899, Accuracy: 43185/50000 (86.37%)\n",
      "Test set: Average loss: 0.4410, Accuracy: 8517/10000 (85.17%)\n",
      "--------------------------------\n",
      "Epoch: 36/200\n",
      "\n",
      "Train set: Average loss: 0.3786, Accuracy: 43404/50000 (86.81%)\n",
      "Test set: Average loss: 0.4407, Accuracy: 8537/10000 (85.37%)\n",
      "--------------------------------\n",
      "Epoch: 37/200\n",
      "\n",
      "Train set: Average loss: 0.3720, Accuracy: 43485/50000 (86.97%)\n",
      "Test set: Average loss: 0.4415, Accuracy: 8527/10000 (85.27%)\n",
      "--------------------------------\n",
      "Epoch: 38/200\n",
      "\n",
      "Train set: Average loss: 0.3677, Accuracy: 43548/50000 (87.10%)\n",
      "Test set: Average loss: 0.4410, Accuracy: 8531/10000 (85.31%)\n",
      "--------------------------------\n",
      "Epoch: 39/200\n",
      "\n",
      "Train set: Average loss: 0.3641, Accuracy: 43603/50000 (87.21%)\n",
      "Test set: Average loss: 0.4360, Accuracy: 8565/10000 (85.65%)\n",
      "--------------------------------\n",
      "Epoch: 40/200\n",
      "\n",
      "Train set: Average loss: 0.3602, Accuracy: 43733/50000 (87.47%)\n",
      "Test set: Average loss: 0.4385, Accuracy: 8563/10000 (85.63%)\n",
      "--------------------------------\n",
      "Epoch: 41/200\n",
      "\n",
      "Train set: Average loss: 0.3573, Accuracy: 43786/50000 (87.57%)\n",
      "Test set: Average loss: 0.4372, Accuracy: 8565/10000 (85.65%)\n",
      "--------------------------------\n",
      "Epoch: 42/200\n",
      "\n",
      "Train set: Average loss: 0.3572, Accuracy: 43709/50000 (87.42%)\n",
      "Test set: Average loss: 0.4336, Accuracy: 8582/10000 (85.82%)\n",
      "--------------------------------\n",
      "Epoch: 43/200\n",
      "\n",
      "Train set: Average loss: 0.3510, Accuracy: 43810/50000 (87.62%)\n",
      "Test set: Average loss: 0.4375, Accuracy: 8576/10000 (85.76%)\n",
      "--------------------------------\n",
      "Epoch: 44/200\n",
      "\n",
      "Train set: Average loss: 0.3464, Accuracy: 43905/50000 (87.81%)\n",
      "Test set: Average loss: 0.4358, Accuracy: 8553/10000 (85.53%)\n",
      "--------------------------------\n",
      "Epoch: 45/200\n",
      "\n",
      "Train set: Average loss: 0.3438, Accuracy: 43974/50000 (87.95%)\n",
      "Test set: Average loss: 0.4366, Accuracy: 8556/10000 (85.56%)\n",
      "--------------------------------\n",
      "Epoch: 46/200\n",
      "\n",
      "Train set: Average loss: 0.3397, Accuracy: 43993/50000 (87.99%)\n",
      "Test set: Average loss: 0.4353, Accuracy: 8592/10000 (85.92%)\n",
      "--------------------------------\n",
      "Epoch: 47/200\n",
      "\n",
      "Train set: Average loss: 0.3330, Accuracy: 44132/50000 (88.26%)\n",
      "Test set: Average loss: 0.4342, Accuracy: 8604/10000 (86.04%)\n",
      "--------------------------------\n",
      "Epoch: 48/200\n",
      "\n",
      "Train set: Average loss: 0.3333, Accuracy: 44163/50000 (88.33%)\n",
      "Test set: Average loss: 0.4333, Accuracy: 8584/10000 (85.84%)\n",
      "--------------------------------\n",
      "Epoch: 49/200\n",
      "\n",
      "Train set: Average loss: 0.3322, Accuracy: 44192/50000 (88.38%)\n",
      "Test set: Average loss: 0.4322, Accuracy: 8590/10000 (85.90%)\n",
      "--------------------------------\n",
      "Epoch: 50/200\n",
      "\n",
      "Train set: Average loss: 0.3233, Accuracy: 44301/50000 (88.60%)\n",
      "Test set: Average loss: 0.4374, Accuracy: 8605/10000 (86.05%)\n",
      "--------------------------------\n",
      "Epoch: 51/200\n",
      "\n",
      "Train set: Average loss: 0.3257, Accuracy: 44314/50000 (88.63%)\n",
      "Test set: Average loss: 0.4341, Accuracy: 8581/10000 (85.81%)\n",
      "--------------------------------\n",
      "Epoch: 52/200\n",
      "\n",
      "Train set: Average loss: 0.3200, Accuracy: 44392/50000 (88.78%)\n",
      "Test set: Average loss: 0.4346, Accuracy: 8589/10000 (85.89%)\n",
      "--------------------------------\n",
      "Epoch: 53/200\n",
      "\n",
      "Train set: Average loss: 0.3225, Accuracy: 44361/50000 (88.72%)\n",
      "Test set: Average loss: 0.4330, Accuracy: 8603/10000 (86.03%)\n",
      "--------------------------------\n",
      "Epoch: 54/200\n",
      "\n",
      "Train set: Average loss: 0.3182, Accuracy: 44435/50000 (88.87%)\n",
      "Test set: Average loss: 0.4348, Accuracy: 8596/10000 (85.96%)\n",
      "--------------------------------\n",
      "Epoch: 55/200\n",
      "\n",
      "Train set: Average loss: 0.3150, Accuracy: 44448/50000 (88.90%)\n",
      "Test set: Average loss: 0.4342, Accuracy: 8597/10000 (85.97%)\n",
      "--------------------------------\n",
      "Epoch: 56/200\n",
      "\n",
      "Train set: Average loss: 0.3099, Accuracy: 44527/50000 (89.05%)\n",
      "Test set: Average loss: 0.4413, Accuracy: 8584/10000 (85.84%)\n",
      "--------------------------------\n",
      "Epoch: 57/200\n",
      "\n",
      "Train set: Average loss: 0.3106, Accuracy: 44547/50000 (89.09%)\n",
      "Test set: Average loss: 0.4364, Accuracy: 8615/10000 (86.15%)\n",
      "--------------------------------\n",
      "Epoch: 58/200\n",
      "\n",
      "Train set: Average loss: 0.3025, Accuracy: 44696/50000 (89.39%)\n",
      "Test set: Average loss: 0.4398, Accuracy: 8582/10000 (85.82%)\n",
      "--------------------------------\n",
      "Epoch: 59/200\n",
      "\n",
      "Train set: Average loss: 0.3018, Accuracy: 44712/50000 (89.42%)\n",
      "Test set: Average loss: 0.4344, Accuracy: 8595/10000 (85.95%)\n",
      "--------------------------------\n",
      "Epoch: 60/200\n",
      "\n",
      "Train set: Average loss: 0.2995, Accuracy: 44609/50000 (89.22%)\n",
      "Test set: Average loss: 0.4454, Accuracy: 8578/10000 (85.78%)\n",
      "--------------------------------\n",
      "Epoch: 61/200\n",
      "\n",
      "Train set: Average loss: 0.2919, Accuracy: 44866/50000 (89.73%)\n",
      "Test set: Average loss: 0.4323, Accuracy: 8610/10000 (86.10%)\n",
      "--------------------------------\n",
      "Epoch: 62/200\n",
      "\n",
      "Train set: Average loss: 0.2867, Accuracy: 44930/50000 (89.86%)\n",
      "Test set: Average loss: 0.4325, Accuracy: 8619/10000 (86.19%)\n",
      "--------------------------------\n",
      "Epoch: 63/200\n",
      "\n",
      "Train set: Average loss: 0.2850, Accuracy: 44980/50000 (89.96%)\n",
      "Test set: Average loss: 0.4315, Accuracy: 8620/10000 (86.20%)\n",
      "--------------------------------\n",
      "Epoch: 64/200\n",
      "\n",
      "Train set: Average loss: 0.2843, Accuracy: 44948/50000 (89.90%)\n",
      "Test set: Average loss: 0.4350, Accuracy: 8617/10000 (86.17%)\n",
      "--------------------------------\n",
      "Epoch: 65/200\n",
      "\n",
      "Train set: Average loss: 0.2786, Accuracy: 45034/50000 (90.07%)\n",
      "Test set: Average loss: 0.4336, Accuracy: 8623/10000 (86.23%)\n",
      "--------------------------------\n",
      "Epoch: 66/200\n",
      "\n",
      "Train set: Average loss: 0.2787, Accuracy: 44974/50000 (89.95%)\n",
      "Test set: Average loss: 0.4349, Accuracy: 8629/10000 (86.29%)\n",
      "--------------------------------\n",
      "Epoch: 67/200\n",
      "\n",
      "Train set: Average loss: 0.2789, Accuracy: 45125/50000 (90.25%)\n",
      "Test set: Average loss: 0.4342, Accuracy: 8609/10000 (86.09%)\n",
      "--------------------------------\n",
      "Epoch: 68/200\n",
      "\n",
      "Train set: Average loss: 0.2778, Accuracy: 45078/50000 (90.16%)\n",
      "Test set: Average loss: 0.4314, Accuracy: 8635/10000 (86.35%)\n",
      "--------------------------------\n",
      "Epoch: 69/200\n",
      "\n",
      "Train set: Average loss: 0.2806, Accuracy: 44987/50000 (89.97%)\n",
      "Test set: Average loss: 0.4327, Accuracy: 8624/10000 (86.24%)\n",
      "--------------------------------\n",
      "Epoch: 70/200\n",
      "\n",
      "Train set: Average loss: 0.2803, Accuracy: 45046/50000 (90.09%)\n",
      "Test set: Average loss: 0.4323, Accuracy: 8600/10000 (86.00%)\n",
      "--------------------------------\n",
      "Epoch: 71/200\n",
      "\n",
      "Train set: Average loss: 0.2789, Accuracy: 45031/50000 (90.06%)\n",
      "Test set: Average loss: 0.4340, Accuracy: 8625/10000 (86.25%)\n",
      "--------------------------------\n",
      "Epoch: 72/200\n",
      "\n",
      "Train set: Average loss: 0.2787, Accuracy: 45087/50000 (90.17%)\n",
      "Test set: Average loss: 0.4357, Accuracy: 8607/10000 (86.07%)\n",
      "--------------------------------\n",
      "Epoch: 73/200\n",
      "\n",
      "Train set: Average loss: 0.2779, Accuracy: 45104/50000 (90.21%)\n",
      "Test set: Average loss: 0.4347, Accuracy: 8605/10000 (86.05%)\n",
      "--------------------------------\n",
      "Epoch: 74/200\n",
      "\n",
      "Train set: Average loss: 0.2784, Accuracy: 45083/50000 (90.17%)\n",
      "Test set: Average loss: 0.4354, Accuracy: 8602/10000 (86.02%)\n",
      "--------------------------------\n",
      "Epoch: 75/200\n",
      "\n",
      "Train set: Average loss: 0.2770, Accuracy: 45059/50000 (90.12%)\n",
      "Test set: Average loss: 0.4362, Accuracy: 8613/10000 (86.13%)\n",
      "--------------------------------\n",
      "Epoch: 76/200\n",
      "\n",
      "Train set: Average loss: 0.2788, Accuracy: 45031/50000 (90.06%)\n",
      "Test set: Average loss: 0.4321, Accuracy: 8617/10000 (86.17%)\n",
      "--------------------------------\n",
      "Epoch: 77/200\n",
      "\n",
      "Train set: Average loss: 0.2751, Accuracy: 45216/50000 (90.43%)\n",
      "Test set: Average loss: 0.4341, Accuracy: 8616/10000 (86.16%)\n",
      "--------------------------------\n",
      "Epoch: 78/200\n",
      "\n",
      "Train set: Average loss: 0.2704, Accuracy: 45259/50000 (90.52%)\n",
      "Test set: Average loss: 0.4325, Accuracy: 8622/10000 (86.22%)\n",
      "--------------------------------\n",
      "Epoch: 79/200\n",
      "\n",
      "Train set: Average loss: 0.2732, Accuracy: 45199/50000 (90.40%)\n",
      "Test set: Average loss: 0.4370, Accuracy: 8607/10000 (86.07%)\n",
      "--------------------------------\n",
      "Epoch: 80/200\n",
      "\n",
      "Train set: Average loss: 0.2741, Accuracy: 45154/50000 (90.31%)\n",
      "Test set: Average loss: 0.4377, Accuracy: 8612/10000 (86.12%)\n",
      "--------------------------------\n",
      "Epoch: 81/200\n",
      "\n",
      "Train set: Average loss: 0.2718, Accuracy: 45237/50000 (90.47%)\n",
      "Test set: Average loss: 0.4369, Accuracy: 8618/10000 (86.18%)\n",
      "--------------------------------\n",
      "Epoch: 82/200\n",
      "\n",
      "Train set: Average loss: 0.2764, Accuracy: 45128/50000 (90.26%)\n",
      "Test set: Average loss: 0.4358, Accuracy: 8629/10000 (86.29%)\n",
      "--------------------------------\n",
      "Epoch: 83/200\n",
      "\n",
      "Train set: Average loss: 0.2754, Accuracy: 45144/50000 (90.29%)\n",
      "Test set: Average loss: 0.4356, Accuracy: 8625/10000 (86.25%)\n",
      "--------------------------------\n",
      "Epoch: 84/200\n",
      "\n",
      "Train set: Average loss: 0.2748, Accuracy: 45151/50000 (90.30%)\n",
      "Test set: Average loss: 0.4378, Accuracy: 8627/10000 (86.27%)\n",
      "--------------------------------\n",
      "Epoch: 85/200\n",
      "\n",
      "Train set: Average loss: 0.2724, Accuracy: 45158/50000 (90.32%)\n",
      "Test set: Average loss: 0.4378, Accuracy: 8629/10000 (86.29%)\n",
      "--------------------------------\n",
      "Epoch: 86/200\n",
      "\n",
      "Train set: Average loss: 0.2672, Accuracy: 45263/50000 (90.53%)\n",
      "Test set: Average loss: 0.4374, Accuracy: 8614/10000 (86.14%)\n",
      "--------------------------------\n",
      "Epoch: 87/200\n",
      "\n",
      "Train set: Average loss: 0.2690, Accuracy: 45247/50000 (90.49%)\n",
      "Test set: Average loss: 0.4380, Accuracy: 8621/10000 (86.21%)\n",
      "--------------------------------\n",
      "Epoch: 88/200\n",
      "\n",
      "Train set: Average loss: 0.2690, Accuracy: 45239/50000 (90.48%)\n",
      "Test set: Average loss: 0.4373, Accuracy: 8617/10000 (86.17%)\n",
      "--------------------------------\n",
      "Epoch: 89/200\n",
      "\n",
      "Train set: Average loss: 0.2701, Accuracy: 45232/50000 (90.46%)\n",
      "Test set: Average loss: 0.4365, Accuracy: 8623/10000 (86.23%)\n",
      "--------------------------------\n",
      "Epoch: 90/200\n",
      "\n",
      "Train set: Average loss: 0.2679, Accuracy: 45342/50000 (90.68%)\n",
      "Test set: Average loss: 0.4387, Accuracy: 8616/10000 (86.16%)\n",
      "--------------------------------\n",
      "Epoch: 91/200\n",
      "\n",
      "Train set: Average loss: 0.2665, Accuracy: 45305/50000 (90.61%)\n",
      "Test set: Average loss: 0.4381, Accuracy: 8622/10000 (86.22%)\n",
      "--------------------------------\n",
      "Epoch: 92/200\n",
      "\n",
      "Train set: Average loss: 0.2647, Accuracy: 45348/50000 (90.70%)\n",
      "Test set: Average loss: 0.4379, Accuracy: 8629/10000 (86.29%)\n",
      "--------------------------------\n",
      "Epoch: 93/200\n",
      "\n",
      "Train set: Average loss: 0.2669, Accuracy: 45242/50000 (90.48%)\n",
      "Test set: Average loss: 0.4359, Accuracy: 8627/10000 (86.27%)\n",
      "--------------------------------\n",
      "Epoch: 94/200\n",
      "\n",
      "Train set: Average loss: 0.2650, Accuracy: 45301/50000 (90.60%)\n",
      "Test set: Average loss: 0.4382, Accuracy: 8625/10000 (86.25%)\n",
      "--------------------------------\n",
      "Epoch: 95/200\n",
      "\n",
      "Train set: Average loss: 0.2678, Accuracy: 45204/50000 (90.41%)\n",
      "Test set: Average loss: 0.4350, Accuracy: 8630/10000 (86.30%)\n",
      "--------------------------------\n",
      "Epoch: 96/200\n",
      "\n",
      "Train set: Average loss: 0.2708, Accuracy: 45177/50000 (90.35%)\n",
      "Test set: Average loss: 0.4382, Accuracy: 8630/10000 (86.30%)\n",
      "--------------------------------\n",
      "Epoch: 97/200\n",
      "\n",
      "Train set: Average loss: 0.2675, Accuracy: 45226/50000 (90.45%)\n",
      "Test set: Average loss: 0.4385, Accuracy: 8634/10000 (86.34%)\n",
      "--------------------------------\n",
      "Epoch: 98/200\n",
      "\n",
      "Train set: Average loss: 0.2659, Accuracy: 45268/50000 (90.54%)\n",
      "Test set: Average loss: 0.4370, Accuracy: 8624/10000 (86.24%)\n",
      "--------------------------------\n",
      "Epoch: 99/200\n",
      "\n",
      "Train set: Average loss: 0.2682, Accuracy: 45228/50000 (90.46%)\n",
      "Test set: Average loss: 0.4381, Accuracy: 8621/10000 (86.21%)\n",
      "--------------------------------\n",
      "Epoch: 100/200\n",
      "\n",
      "Train set: Average loss: 0.2702, Accuracy: 45209/50000 (90.42%)\n",
      "Test set: Average loss: 0.4372, Accuracy: 8619/10000 (86.19%)\n",
      "--------------------------------\n",
      "Epoch: 101/200\n",
      "\n",
      "Train set: Average loss: 0.2683, Accuracy: 45216/50000 (90.43%)\n",
      "Test set: Average loss: 0.4375, Accuracy: 8622/10000 (86.22%)\n",
      "--------------------------------\n",
      "Epoch: 102/200\n",
      "\n",
      "Train set: Average loss: 0.2673, Accuracy: 45246/50000 (90.49%)\n",
      "Test set: Average loss: 0.4368, Accuracy: 8631/10000 (86.31%)\n",
      "--------------------------------\n",
      "Epoch: 103/200\n",
      "\n",
      "Train set: Average loss: 0.2655, Accuracy: 45300/50000 (90.60%)\n",
      "Test set: Average loss: 0.4368, Accuracy: 8629/10000 (86.29%)\n",
      "--------------------------------\n",
      "Epoch: 104/200\n",
      "\n",
      "Train set: Average loss: 0.2676, Accuracy: 45244/50000 (90.49%)\n",
      "Test set: Average loss: 0.4357, Accuracy: 8632/10000 (86.32%)\n",
      "--------------------------------\n",
      "Epoch: 105/200\n",
      "\n",
      "Train set: Average loss: 0.2682, Accuracy: 45188/50000 (90.38%)\n",
      "Test set: Average loss: 0.4368, Accuracy: 8632/10000 (86.32%)\n",
      "--------------------------------\n",
      "Epoch: 106/200\n",
      "\n",
      "Train set: Average loss: 0.2700, Accuracy: 45134/50000 (90.27%)\n",
      "Test set: Average loss: 0.4368, Accuracy: 8622/10000 (86.22%)\n",
      "--------------------------------\n",
      "Epoch: 107/200\n",
      "\n",
      "Train set: Average loss: 0.2672, Accuracy: 45278/50000 (90.56%)\n",
      "Test set: Average loss: 0.4352, Accuracy: 8628/10000 (86.28%)\n",
      "--------------------------------\n",
      "Epoch: 108/200\n",
      "\n",
      "Train set: Average loss: 0.2680, Accuracy: 45204/50000 (90.41%)\n",
      "Test set: Average loss: 0.4372, Accuracy: 8636/10000 (86.36%)\n",
      "--------------------------------\n",
      "Epoch: 109/200\n",
      "\n",
      "Train set: Average loss: 0.2656, Accuracy: 45270/50000 (90.54%)\n",
      "Test set: Average loss: 0.4364, Accuracy: 8628/10000 (86.28%)\n",
      "--------------------------------\n",
      "Epoch: 110/200\n",
      "\n",
      "Train set: Average loss: 0.2689, Accuracy: 45198/50000 (90.40%)\n",
      "Test set: Average loss: 0.4384, Accuracy: 8627/10000 (86.27%)\n",
      "--------------------------------\n",
      "Epoch: 111/200\n",
      "\n",
      "Train set: Average loss: 0.2630, Accuracy: 45346/50000 (90.69%)\n",
      "Test set: Average loss: 0.4385, Accuracy: 8624/10000 (86.24%)\n",
      "--------------------------------\n",
      "Epoch: 112/200\n",
      "\n",
      "Train set: Average loss: 0.2690, Accuracy: 45258/50000 (90.52%)\n",
      "Test set: Average loss: 0.4367, Accuracy: 8619/10000 (86.19%)\n",
      "--------------------------------\n",
      "Epoch: 113/200\n",
      "\n",
      "Train set: Average loss: 0.2649, Accuracy: 45280/50000 (90.56%)\n",
      "Test set: Average loss: 0.4356, Accuracy: 8637/10000 (86.37%)\n",
      "--------------------------------\n",
      "Epoch: 114/200\n",
      "\n",
      "Train set: Average loss: 0.2642, Accuracy: 45352/50000 (90.70%)\n",
      "Test set: Average loss: 0.4371, Accuracy: 8617/10000 (86.17%)\n",
      "--------------------------------\n",
      "Epoch: 115/200\n",
      "\n",
      "Train set: Average loss: 0.2657, Accuracy: 45293/50000 (90.59%)\n",
      "Test set: Average loss: 0.4369, Accuracy: 8633/10000 (86.33%)\n",
      "--------------------------------\n",
      "Epoch: 116/200\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    print(f'Epoch: {epoch}/{epochs}')\n",
    "    train(model, device, train_loader, optimizer, criterion)  # Your train function\n",
    "    evaluate(model, device, test_loader, criterion)  # Your evaluate function\n",
    "    scheduler.step()  # Update the learning rate\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07f0c4",
   "metadata": {
    "id": "0c07f0c4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32cb4c",
   "metadata": {
    "id": "6d32cb4c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dabddd9",
   "metadata": {
    "id": "4dabddd9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ecd238",
   "metadata": {
    "id": "68ecd238"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac63a0",
   "metadata": {
    "id": "d7ac63a0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d578a",
   "metadata": {
    "id": "694d578a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
